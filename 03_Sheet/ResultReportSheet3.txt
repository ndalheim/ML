ML - Ken Moller, Nadine Dalheimer, Alexander Kindl
Sheet 03

Task 1)

a)
Random Forest stellt eine Erweiterung von Bagging (bootstrap aggregation) dar. Bei Random Forest werden die Trainingsdatensätze zunächst mit Bagging erzeugt (Sampling with replacement). Random Forest fügt nun einen zusätzlichen Schritt im Lernprozess eines Entscheidungsbaumes ein. Nachdem ein Feature (nächster Knoten im Baum) gewählt wurde, wird das neue Datenset nochmals gefiltert. Dieses mal jedoch nach Attributen. Hier werden nach Zufall Attribute entfernt. Dieser neu enstandene Datensatz wird für den weiteren Algorithmus verwendet.


b)

PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 1 1
Bagging iteration : 1
Num RF iterations : 1
Average accuracy : 84.8639455782313
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 5 1
Bagging iteration : 5
Num RF iterations : 1
Average accuracy : 92.34693877551021
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 10 1
Bagging iteration : 10
Num RF iterations : 1
Average accuracy : 93.02721088435374
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 20 1
Bagging iteration : 20
Num RF iterations : 1
Average accuracy : 92.00680272108843
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 30 1
Bagging iteration : 30
Num RF iterations : 1
Average accuracy : 93.19727891156462
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 40 1
Bagging iteration : 40
Num RF iterations : 1
Average accuracy : 92.17687074829932
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 50 1
Bagging iteration : 50
Num RF iterations : 1
Average accuracy : 94.5578231292517

Die accuracy ist nicht ganz stabil und fluktuiert, aber im Mittel (Aufgabenteil c)) ist sichtbar, dass diese im Durchsnitt stabiler ist.


c)

PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 1 10
Bagging iteration : 1
Num RF iterations : 10
Average accuracy : 86.39455782312926
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 5 10
Bagging iteration : 5
Num RF iterations : 10
Average accuracy : 91.156462585034
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 10 10
Bagging iteration : 10
Num RF iterations : 10
Average accuracy : 92.34693877551022
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 20 10
Bagging iteration : 20
Num RF iterations : 10
Average accuracy : 92.00680272108843
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 30 10
Bagging iteration : 30
Num RF iterations : 10
Average accuracy : 93.5374149659864
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 40 10
Bagging iteration : 40
Num RF iterations : 10
Average accuracy : 94.7278911564626
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 50 10
Bagging iteration : 50
Num RF iterations : 10
Average accuracy : 93.36734693877553
PS C:\Users\ken\uni\08_UNI_SS_18\ML\03_Sheet> java -jar .\Task1.jar ..\data\car.arff 6 100 10
Bagging iteration : 100
Num RF iterations : 10
Average accuracy : 93.36734693877553

Die Erkenntnis ist, dass die Accuracy stabiler wird. Sie scheint außerdem ein Maximum bei 40 Bagging iterations zu hhaben.

Unser Baum vom letzten Blatt hate bei Baumtiefe 5 eine maximale Accuracy von 0.9026. Durch Bagging konnte die Accuracy also um ca. 4 - 5 % gesteigert werden.



Task 2)

java -jar .\Task2.jar ..\data\car.arff 6 50 10 <max depth>

Tiefe 1 :
Accuracy mean : 0.7052631578947368
Accuracy std. var. : 1.6066481994459828E-4

Tiefe 2:
Accuracy mean : 0.5252631578947369
Accuracy std. var. : 0.09194041243459525

Tiefe 3:
Accuracy mean : 0.8277192982456141
Accuracy std. var. : 1.4638350261618948E-4

Tiefe 4:
Accuracy mean : 0.8903508771929826
Accuracy std. var. : 2.2606955986457359E-4

Tiefe 5:
Accuracy mean : 0.9043859649122806
Accuracy std. var. : 8.879655278547234E-5

Tiefe 6:
Accuracy mean : 0.904561403508772
Accuracy std. var. : 2.64204370575562E-4


Ab 6 ist redundant, da der Baum maximal Tiefe 6 erreicht.

Boosting bringt ab einer gewissen Baumtiefe eine Mehrwert, kann jedoch auch bei zu geringer Tiefe schlechtere Ergebnisse liefern.



Task 3)

In diesem Fall ist der Model-Error bereits in der ersten iteration 0 und boosting bricht ab. Somit wird nur ein Model verwendet und boosting bringt keine Verbesserung.

Task 4)
Bei Bootstrap ist 0.632 die Wahrscheinlichkeit eines Datenpunktes im Trainingsdatensatz zu landen. Dies gilt jedoch nur für große n.




